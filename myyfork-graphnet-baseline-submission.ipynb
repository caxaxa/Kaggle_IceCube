{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GraphNeT Baseline Submission\n<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/graphnet-team/graphnet/main/assets/identity/graphnet-logo-and-wordmark.png\" width=\"600\" height=\"600\" />\n\nThis notebook submits predictions from the public pre-trained dynedge to the leaderboard. ","metadata":{}},{"cell_type":"code","source":"# Move software to working disk\n!rm  -r software\n!scp -r /kaggle/input/graphnet-and-dependencies/software .\n\n# Install dependencies\n!pip install /kaggle/working/software/dependencies/torch-1.11.0+cu115-cp37-cp37m-linux_x86_64.whl\n!pip install /kaggle/working/software/dependencies/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl\n!pip install /kaggle/working/software/dependencies/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl\n!pip install /kaggle/working/software/dependencies/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl\n!pip install /kaggle/working/software/dependencies/torch_geometric-2.0.4.tar.gz\n\n# Install GraphNeT\n!cd software/graphnet;pip install --no-index --find-links=\"/kaggle/working/software/dependencies\" -e .[torch]\n\n# Append to PATH\nimport sys\nsys.path.append('/kaggle/working/software/graphnet/src')","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:26:06.187391Z","iopub.execute_input":"2023-04-16T04:26:06.187780Z","iopub.status.idle":"2023-04-16T04:29:58.049482Z","shell.execute_reply.started":"2023-04-16T04:26:06.187746Z","shell.execute_reply":"2023-04-16T04:29:58.048020Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Processing ./software/dependencies/torch-1.11.0+cu115-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.11.0+cu115) (4.1.1)\ntorch is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n\u001b[0mProcessing ./software/dependencies/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl\ntorch-cluster is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n\u001b[0mProcessing ./software/dependencies/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl\ntorch-scatter is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n\u001b[0mProcessing ./software/dependencies/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-sparse==0.6.13) (1.7.3)\nRequirement already satisfied: numpy<1.23.0,>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from scipy->torch-sparse==0.6.13) (1.21.6)\ntorch-sparse is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n\u001b[0mProcessing ./software/dependencies/torch_geometric-2.0.4.tar.gz\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (4.64.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (1.21.6)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (1.7.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (1.3.5)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (2.28.1)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (3.0.9)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (1.0.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->torch-geometric==2.0.4) (2.1.2)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric==2.0.4) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric==2.0.4) (2022.1)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric==2.0.4) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric==2.0.4) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric==2.0.4) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric==2.0.4) (1.26.14)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric==2.0.4) (1.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric==2.0.4) (3.1.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->torch-geometric==2.0.4) (1.15.0)\nBuilding wheels for collected packages: torch-geometric\n  Building wheel for torch-geometric (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=69a3df68804bd101f49603563594b04f4495e84a725e0eb6162f42c5b9af9954\n  Stored in directory: /root/.cache/pip/wheels/c0/33/a3/07aa146f758cd91ebee36268011873ae31c2cfc59dec089e04\nSuccessfully built torch-geometric\nInstalling collected packages: torch-geometric\n  Attempting uninstall: torch-geometric\n    Found existing installation: torch-geometric 2.0.4\n    Uninstalling torch-geometric-2.0.4:\n      Successfully uninstalled torch-geometric-2.0.4\nSuccessfully installed torch-geometric-2.0.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n\u001b[0mLooking in links: /kaggle/working/software/dependencies\nObtaining file:///kaggle/working/software/graphnet\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: awkward<2.0,>=1.8 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.8.0)\nRequirement already satisfied: colorlog>=6.6 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (6.7.0)\nRequirement already satisfied: configupdater in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (3.1.1)\nRequirement already satisfied: dill>=0.3 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (0.3.6)\nRequirement already satisfied: matplotlib>=3.5 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (3.5.2)\nRequirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.21.6)\nRequirement already satisfied: pandas>=1.3 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.3.5)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (5.0.0)\nRequirement already satisfied: pydantic in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.8.2)\nRequirement already satisfied: ruamel.yaml in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (0.17.21)\nRequirement already satisfied: scikit_learn>=1.0 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.0.2)\nRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.7.3)\nRequirement already satisfied: sqlalchemy>=1.4 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.4.39)\nRequirement already satisfied: timer>=0.2 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (0.2.2)\nRequirement already satisfied: tqdm>=4.64 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (4.64.0)\nRequirement already satisfied: wandb>=0.12 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (0.12.21)\nRequirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.11.0+cu115)\nRequirement already satisfied: torch-cluster>=1.6 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.6.0)\nRequirement already satisfied: torch-scatter>=2.0 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (2.0.9)\nRequirement already satisfied: torch-sparse>=0.6 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (0.6.13)\nRequirement already satisfied: torch-geometric>=2.0 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (2.0.4)\nRequirement already satisfied: pytorch-lightning>=1.6 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.9.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from awkward<2.0,>=1.8->graphnet==0.2.4+77.g57fd0f3) (59.8.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (1.4.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (23.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (0.11.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (2.8.2)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (9.1.1)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (3.0.9)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (4.33.3)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.3->graphnet==0.2.4+77.g57fd0f3) (2022.1)\nRequirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (0.11.1)\nRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (4.1.1)\nRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (6.0)\nRequirement already satisfied: lightning-utilities>=0.4.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (0.6.0.post0)\nRequirement already satisfied: fsspec[http]>2021.06.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (2023.1.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>=1.0->graphnet==0.2.4+77.g57fd0f3) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>=1.0->graphnet==0.2.4+77.g57fd0f3) (1.0.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.4->graphnet==0.2.4+77.g57fd0f3) (6.0.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.4->graphnet==0.2.4+77.g57fd0f3) (1.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (2.28.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (3.1.2)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (0.4.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (1.3.2)\nRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (1.0.11)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (1.14.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (5.9.1)\nRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (3.19.4)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (3.1.27)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (8.1.3)\nRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (1.15.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (0.1.2)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (2.3)\nRequirement already satisfied: ruamel.yaml.clib>=0.2.6 in /opt/conda/lib/python3.7/site-packages (from ruamel.yaml->graphnet==0.2.4+77.g57fd0f3) (0.2.7)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.7/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (3.8.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (4.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->sqlalchemy>=1.4->graphnet==0.2.4+77.g57fd0f3) (3.8.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (2.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (2.1.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (0.13.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (1.3.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (6.0.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (21.4.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (1.7.2)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (4.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (1.2.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (3.0.5)\nInstalling collected packages: graphnet\n  Attempting uninstall: graphnet\n    Found existing installation: graphnet 0.2.4+77.g57fd0f3\n    Uninstalling graphnet-0.2.4+77.g57fd0f3:\n      Successfully uninstalled graphnet-0.2.4+77.g57fd0f3\n  Running setup.py develop for graphnet\nSuccessfully installed graphnet-0.2.4+77.g57fd0f3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pyarrow.parquet as pq\nimport sqlite3\nimport pandas as pd\nimport sqlalchemy\nfrom tqdm import tqdm\nimport os\nfrom typing import Any, Dict, List, Optional\nimport numpy as np\n\nfrom graphnet.data.sqlite.sqlite_utilities import create_table\n\ndef load_input(meta_batch: pd.DataFrame, input_data_folder: str) -> pd.DataFrame:\n        \"\"\"\n        Will load the corresponding detector readings associated with the meta data batch.\n        \"\"\"\n        batch_id = pd.unique(meta_batch['batch_id'])\n\n        assert len(batch_id) == 1, \"contains multiple batch_ids. Did you set the batch_size correctly?\"\n        \n        detector_readings = pd.read_parquet(path = f'{input_data_folder}/batch_{batch_id[0]}.parquet')\n        sensor_positions = geometry_table.loc[detector_readings['sensor_id'], ['x', 'y', 'z']]\n        sensor_positions.index = detector_readings.index\n\n        for column in sensor_positions.columns:\n            if column not in detector_readings.columns:\n                detector_readings[column] = sensor_positions[column]\n\n        detector_readings['auxiliary'] = detector_readings['auxiliary'].replace({True: 1, False: 0})\n        return detector_readings.reset_index()\n\ndef add_to_table(database_path: str,\n                      df: pd.DataFrame,\n                      table_name:  str,\n                      is_primary_key: bool,\n                      ) -> None:\n    \"\"\"Writes meta data to sqlite table. \n\n    Args:\n        database_path (str): the path to the database file.\n        df (pd.DataFrame): the dataframe that is being written to table.\n        table_name (str, optional): The name of the meta table. Defaults to 'meta_table'.\n        is_primary_key(bool): Must be True if each row of df corresponds to a unique event_id. Defaults to False.\n    \"\"\"\n    try:\n        print(database_path)\n        create_table(   columns=  df.columns,\n                        database_path = database_path, \n                        table_name = table_name,\n                        integer_primary_key= is_primary_key,\n                        index_column = 'event_id')\n    except sqlite3.OperationalError as e:\n        if 'already exists' in str(e):\n            pass\n        else:\n            raise e\n    engine = sqlalchemy.create_engine(\"sqlite:///\" + database_path)\n    df.to_sql(table_name, con=engine, index=False, if_exists=\"append\", chunksize = 200000)\n    engine.dispose()\n    return\n\ndef convert_to_sqlite(meta_data_path: str,\n                      database_path: str,\n                      input_data_folder: str,\n                      batch_size: int = 200000) -> None:\n    \"\"\"Converts a selection of the Competition's parquet files to a single sqlite database.\n\n    Args:\n        meta_data_path (str): Path to the meta data file.\n        batch_size (int): the number of rows extracted from meta data file at a time. Keep low for memory efficiency.\n        database_path (str): path to database. E.g. '/my_folder/data/my_new_database.db'\n        input_data_folder (str): folder containing the parquet input files.\n        accepted_batch_ids (List[int]): The batch_ids you want converted. Defaults to None (all batches will be converted)\n    \"\"\"\n    meta_data_iter = pq.ParquetFile(meta_data_path).iter_batches(batch_size = batch_size)\n    \n    if not database_path.endswith('.db'):\n        database_path = database_path+'.db'\n        \n    converted_batches = [] \n    progress_bar = tqdm(total = None)\n    for meta_data_batch in meta_data_iter:\n        unique_batch_ids = pd.unique(meta_data_batch['event_id']).tolist()\n        meta_data_batch  = meta_data_batch.to_pandas()\n        add_to_table(database_path = database_path,\n                    df = meta_data_batch,\n                    table_name='meta_table',\n                    is_primary_key= True)\n        pulses = load_input(meta_batch=meta_data_batch, input_data_folder= input_data_folder)\n        del meta_data_batch # memory\n        add_to_table(database_path = database_path,\n                    df = pulses,\n                    table_name='pulse_table',\n                    is_primary_key= False)\n        del pulses # memory\n        progress_bar.update(1)\n    progress_bar.close()\n    del meta_data_iter # memory\n    print(f'Conversion Complete!. Database available at\\n {database_path}')","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:52:18.473156Z","iopub.execute_input":"2023-04-16T04:52:18.473612Z","iopub.status.idle":"2023-04-16T04:52:18.490649Z","shell.execute_reply.started":"2023-04-16T04:52:18.473575Z","shell.execute_reply":"2023-04-16T04:52:18.489299Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"!rm '/kaggle/working/test_database.db'\ninput_data_folder = '/kaggle/input/icecube-neutrinos-in-deep-ice/test'\ngeometry_table = pd.read_csv('/kaggle/input/icecube-neutrinos-in-deep-ice/sensor_geometry.csv')\nmeta_data_path = '/kaggle/input/icecube-neutrinos-in-deep-ice/test_meta.parquet'\n\ndatabase_path = '/kaggle/working/test_database'\nconvert_to_sqlite(meta_data_path,\n                  database_path=database_path,\n                  input_data_folder=input_data_folder)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:52:18.753884Z","iopub.execute_input":"2023-04-16T04:52:18.756439Z","iopub.status.idle":"2023-04-16T04:52:19.957741Z","shell.execute_reply.started":"2023-04-16T04:52:18.756396Z","shell.execute_reply":"2023-04-16T04:52:19.956490Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"/kaggle/working/test_database.db\n/kaggle/working/test_database.db\n","output_type":"stream"},{"name":"stderr","text":"1it [00:00, 10.82it/s]","output_type":"stream"},{"name":"stdout","text":"Conversion Complete!. Database available at\n /kaggle/working/test_database.db\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"from pytorch_lightning.callbacks import EarlyStopping\nfrom torch.optim.adam import Adam\nfrom graphnet.data.constants import FEATURES, TRUTH\nfrom graphnet.models import StandardModel\nfrom graphnet.models.detector.icecube import IceCubeKaggle\nfrom graphnet.models.gnn import DynEdge\nfrom graphnet.models.graph_builders import KNNGraphBuilder\nfrom graphnet.models.task.reconstruction import DirectionReconstructionWithKappa, ZenithReconstructionWithKappa, AzimuthReconstructionWithKappa\nfrom graphnet.training.callbacks import ProgressBar, PiecewiseLinearLR\nfrom graphnet.training.loss_functions import VonMisesFisher3DLoss, VonMisesFisher2DLoss\nfrom graphnet.training.labels import Direction\nfrom graphnet.training.utils import make_dataloader\nfrom graphnet.utilities.logging import get_logger\nfrom pytorch_lightning import Trainer\nimport pandas as pd\n\nlogger = get_logger()\n\ndef build_model(config: Dict[str,Any], train_dataloader: Any) -> StandardModel:\n    \"\"\"Builds GNN from config\"\"\"\n    # Building model\n    detector = IceCubeKaggle(\n        graph_builder=KNNGraphBuilder(nb_nearest_neighbours=8),\n    )\n    gnn = DynEdge(\n        nb_inputs=detector.nb_outputs,\n        global_pooling_schemes=[\"min\", \"max\", \"mean\"],\n    )\n\n    if config[\"target\"] == 'direction':\n        task = DirectionReconstructionWithKappa(\n            hidden_size=gnn.nb_outputs,\n            target_labels=config[\"target\"],\n            loss_function=VonMisesFisher3DLoss(),\n        )\n        prediction_columns = [config[\"target\"] + \"_x\", \n                              config[\"target\"] + \"_y\", \n                              config[\"target\"] + \"_z\", \n                              config[\"target\"] + \"_kappa\" ]\n        additional_attributes = ['zenith', 'azimuth', 'event_id']\n\n    model = StandardModel(\n        detector=detector,\n        gnn=gnn,\n        tasks=[task],\n        optimizer_class=Adam,\n        optimizer_kwargs={\"lr\": 5e-04, \"eps\": 5e-04},\n        scheduler_class=PiecewiseLinearLR,\n        scheduler_kwargs={\n            \"milestones\": [\n                0,\n                len(train_dataloader) / 2,\n                len(train_dataloader) * config[\"fit\"][\"max_epochs\"],\n            ],\n            \"factors\": [1e-02, 1, 1e-02],\n        },\n        scheduler_config={\n            \"interval\": \"step\",\n        },\n    )\n    model.prediction_columns = prediction_columns\n    model.additional_attributes = additional_attributes\n    \n    return model\n\ndef build_model1(config: Dict[str,Any], train_dataloader: Any) -> StandardModel:\n    \"\"\"Builds GNN from config\"\"\"\n    # Building model\n    detector = IceCubeKaggle(\n        graph_builder=KNNGraphBuilder(nb_nearest_neighbours=12),\n    )\n    gnn = DynEdge(\n        nb_inputs=detector.nb_outputs,\n        global_pooling_schemes=[\"min\", \"max\", \"mean\"],\n    )\n\n    if config[\"target\"] == 'direction':\n        task = DirectionReconstructionWithKappa(\n            hidden_size=gnn.nb_outputs,\n            target_labels=config[\"target\"],\n            loss_function=VonMisesFisher3DLoss(),\n        )\n        prediction_columns = [config[\"target\"] + \"_x\", \n                              config[\"target\"] + \"_y\", \n                              config[\"target\"] + \"_z\", \n                              config[\"target\"] + \"_kappa\" ]\n        additional_attributes = ['zenith', 'azimuth', 'event_id']\n\n    model = StandardModel(\n        detector=detector,\n        gnn=gnn,\n        tasks=[task],\n        optimizer_class=Adam,\n        optimizer_kwargs={\"lr\": 5e-04, \"eps\": 5e-04},\n        scheduler_class=PiecewiseLinearLR,\n        scheduler_kwargs={\n            \"milestones\": [\n                0,\n                len(train_dataloader) / 2,\n                len(train_dataloader) * config[\"fit\"][\"max_epochs\"],\n            ],\n            \"factors\": [1e-02, 1, 1e-02],\n        },\n        scheduler_config={\n            \"interval\": \"step\",\n        },\n    )\n    model.prediction_columns = prediction_columns\n    model.additional_attributes = additional_attributes\n    \n    return model\n\n\ndef load_pretrained_model(config: Dict[str,Any], state_dict_path: str = '/kaggle/input/dynedge-pretrained/dynedge_pretrained_batch_1_to_50/state_dict.pth') -> StandardModel:\n    train_dataloader, _ = make_dataloaders(config = config)\n    model = build_model(config = config, \n                        train_dataloader = train_dataloader)\n    #model._inference_trainer = Trainer(config['fit'])\n    model.load_state_dict(state_dict_path)\n    model.prediction_columns = [config[\"target\"] + \"_x\", \n                              config[\"target\"] + \"_y\", \n                              config[\"target\"] + \"_z\", \n                              config[\"target\"] + \"_kappa\" ]\n    model.additional_attributes = ['event_id'] #'zenith', 'azimuth',  not available in test data\n    return model\n\ndef load_pretrained_model2(config: Dict[str,Any], state_dict_path: str = '/kaggle/input/icecubemodel/model(1).pth') -> StandardModel:\n    train_dataloader, _ = make_dataloaders(config = config)\n    model = build_model1(config = config, \n                        train_dataloader = train_dataloader)\n    #model._inference_trainer = Trainer(config['fit'])\n    model.load_state_dict(state_dict_path)\n    model.prediction_columns = [config[\"target\"] + \"_x\", \n                              config[\"target\"] + \"_y\", \n                              config[\"target\"] + \"_z\", \n                              config[\"target\"] + \"_kappa\" ]\n    model.additional_attributes = ['event_id'] #'zenith', 'azimuth',  not available in test data\n    return model\n\n\ndef make_dataloaders(config: Dict[str, Any]) -> List[Any]:\n    \"\"\"Constructs training and validation dataloaders for training with early stopping.\"\"\"\n    \n    train_dataloader = make_dataloader(db = config['path'],\n                                            selection = pd.read_csv(config['train_selection'])[config['index_column']].ravel().tolist() if config['train_selection'] else None,\n                                            pulsemaps = config['pulsemap'],\n                                            features = features,\n                                            truth = truth,\n                                            batch_size = config['batch_size'],\n                                            num_workers = config['num_workers'],\n                                            shuffle = True,\n                                            labels = {'direction': Direction()},\n                                            index_column = config['index_column'],\n                                            truth_table = config['truth_table'],\n                                            )\n    \n    validate_dataloader = make_dataloader(db = config['path'],\n                                            selection = pd.read_csv(config['validate_selection'])[config['index_column']].ravel().tolist() if config['validate_selection'] else None,\n                                            pulsemaps = config['pulsemap'],\n                                            features = features,\n                                            truth = truth,\n                                            batch_size = config['batch_size'],\n                                            num_workers = config['num_workers'],\n                                            shuffle = False,\n                                            labels = {'direction': Direction()},\n                                            index_column = config['index_column'],\n                                            truth_table = config['truth_table'],\n                                          \n                                            )\n    return train_dataloader, validate_dataloader\n\ndef inference(models, config: Dict[str, Any]) -> pd.DataFrame:\n    \"\"\"Applies model to the database specified in config['inference_database_path'] and saves results to disk.\"\"\"\n    # Make Dataloader\n    test_dataloader = make_dataloader(db = config['inference_database_path'],\n                                            selection = None, # Entire database\n                                            pulsemaps = config['pulsemap'],\n                                            features = features,\n                                            truth = truth,\n                                            batch_size = config['batch_size'],\n                                            num_workers = config['num_workers'],\n                                            shuffle = False,\n                                            labels = None, # Cannot make labels in test data\n                                            index_column = config['index_column'],\n                                            truth_table = config['truth_table'],\n                                            )\n    \n    # Get predictions\n    results1 = models[0].predict_as_dataframe(\n        gpus = [0],\n        dataloader = test_dataloader,\n        prediction_columns=models[0].prediction_columns,\n        additional_attributes=['event_id']\n    )\n    \n    print(results1)\n    \n    results2 = models[1].predict_as_dataframe(\n        gpus = [0],\n        dataloader = test_dataloader,\n        prediction_columns=models[1].prediction_columns,\n        additional_attributes=['event_id']\n    )\n\n    results1[\"direction_x\"] = 0.9 * results1[\"direction_x\"] + 0.1 * results2[\"direction_x\"]\n    results1[\"direction_y\"] = 0.9 * results1[\"direction_y\"] + 0.1 * results2[\"direction_y\"]\n    results1[\"direction_z\"] = 0.9 * results1[\"direction_z\"] + 0.1 * results2[\"direction_z\"]\n    results1[\"direction_kappa\"] = 0.9 * results1[\"direction_kappa\"] + 0.1 * results2[\"direction_kappa\"]\n\n    return results1\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:52:19.962056Z","iopub.execute_input":"2023-04-16T04:52:19.962388Z","iopub.status.idle":"2023-04-16T04:52:19.999138Z","shell.execute_reply.started":"2023-04-16T04:52:19.962353Z","shell.execute_reply":"2023-04-16T04:52:19.997960Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def prepare_dataframe(df, angle_post_fix = '_reco', vec_post_fix = '') -> pd.DataFrame:\n    r = np.sqrt(df['direction_x'+ vec_post_fix]**2 + df['direction_y'+ vec_post_fix]**2 + df['direction_z' + vec_post_fix]**2)\n    df['zenith' + angle_post_fix] = np.arccos(df['direction_z'+ vec_post_fix]/r)\n    df['azimuth'+ angle_post_fix] = np.arctan2(df['direction_y'+ vec_post_fix],df['direction_x' + vec_post_fix]) #np.sign(results['true_y'])*np.arccos((results['true_x'])/(np.sqrt(results['true_x']**2 + results['true_y']**2)))\n    df['azimuth'+ angle_post_fix][df['azimuth'  + angle_post_fix]<0] = df['azimuth'  + angle_post_fix][df['azimuth'  +  angle_post_fix]<0] + 2*np.pi \n\n    drop_these_columns = []\n    for column in results.columns:\n        if column not in ['event_id', 'zenith', 'azimuth']:\n            drop_these_columns.append(column)\n    return df.drop(columns = drop_these_columns).iloc[:,[0,2,1]].set_index('event_id')\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:52:20.001036Z","iopub.execute_input":"2023-04-16T04:52:20.003042Z","iopub.status.idle":"2023-04-16T04:52:20.013845Z","shell.execute_reply.started":"2023-04-16T04:52:20.003005Z","shell.execute_reply":"2023-04-16T04:52:20.012672Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Constants\nfeatures = FEATURES.KAGGLE\ntruth = TRUTH.KAGGLE\n\n# Configuration\nconfig = {\n        \"path\": '/kaggle/working/test_database.db',\n        \"inference_database_path\": '/kaggle/working/test_database.db',\n        \"pulsemap\": 'pulse_table',\n        \"truth_table\": 'meta_table',\n        \"features\": features,\n        \"truth\": truth,\n        \"index_column\": 'event_id',\n        \"run_name_tag\": 'graphnet_baseline_submission',\n        \"batch_size\": 250,\n        \"num_workers\": 2,\n        \"target\": 'direction',\n        \"early_stopping_patience\": 5,\n        \"fit\": {\n                \"max_epochs\": 70,\n                \"gpus\": [0],\n                \"distribution_strategy\": None,\n                },\n        'train_selection': None,\n        'validate_selection':  None,\n        'test_selection': None,\n        'base_dir': 'training'\n}\nmodels = [load_pretrained_model(config = config), load_pretrained_model(config = config)]\n\nresults = inference(models, config)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:52:20.016696Z","iopub.execute_input":"2023-04-16T04:52:20.017410Z","iopub.status.idle":"2023-04-16T04:52:20.959061Z","shell.execute_reply.started":"2023-04-16T04:52:20.017371Z","shell.execute_reply":"2023-04-16T04:52:20.957017Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-04-16 04:52:20 - SQLiteDataset.warning - Removing the following (missing) truth variables: zenith, azimuth\u001b[0m\n\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-04-16 04:52:20 - SQLiteDataset.warning - Removing the following (missing) truth variables: zenith, azimuth\u001b[0m\n\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-04-16 04:52:20 - SQLiteDataset.warning - Removing the following (missing) truth variables: zenith, azimuth\u001b[0m\n\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-04-16 04:52:20 - SQLiteDataset.warning - Removing the following (missing) truth variables: zenith, azimuth\u001b[0m\n\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-04-16 04:52:20 - SQLiteDataset.warning - Removing the following (missing) truth variables: zenith, azimuth\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a75838769304dbc96aa2e31ef6dece3"}},"metadata":{}},{"name":"stdout","text":"   direction_x  direction_y  direction_z  direction_kappa  event_id\n0     0.987893    -0.148705     0.044196         2.946704    2092.0\n1    -0.551074    -0.254085    -0.794832         6.980600    7344.0\n2    -0.164367    -0.986232     0.018184        86.766502    9482.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8687ca0e70924ee3994008a9500b4db7"}},"metadata":{}}]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:52:20.967641Z","iopub.execute_input":"2023-04-16T04:52:20.968623Z","iopub.status.idle":"2023-04-16T04:52:21.005967Z","shell.execute_reply.started":"2023-04-16T04:52:20.968557Z","shell.execute_reply":"2023-04-16T04:52:21.004462Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"   direction_x  direction_y  direction_z  direction_kappa  event_id\n0     0.987893    -0.148705     0.044196         2.946704    2092.0\n1    -0.551074    -0.254085    -0.794832         6.980601    7344.0\n2    -0.164367    -0.986232     0.018184        86.766502    9482.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>direction_x</th>\n      <th>direction_y</th>\n      <th>direction_z</th>\n      <th>direction_kappa</th>\n      <th>event_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.987893</td>\n      <td>-0.148705</td>\n      <td>0.044196</td>\n      <td>2.946704</td>\n      <td>2092.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.551074</td>\n      <td>-0.254085</td>\n      <td>-0.794832</td>\n      <td>6.980601</td>\n      <td>7344.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.164367</td>\n      <td>-0.986232</td>\n      <td>0.018184</td>\n      <td>86.766502</td>\n      <td>9482.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submission_df = prepare_dataframe(results, angle_post_fix = '')","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:52:21.379006Z","iopub.execute_input":"2023-04-16T04:52:21.379430Z","iopub.status.idle":"2023-04-16T04:52:21.395407Z","shell.execute_reply.started":"2023-04-16T04:52:21.379395Z","shell.execute_reply":"2023-04-16T04:52:21.393840Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"#submission_df.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:52:24.265118Z","iopub.execute_input":"2023-04-16T04:52:24.265504Z","iopub.status.idle":"2023-04-16T04:52:24.270591Z","shell.execute_reply.started":"2023-04-16T04:52:24.265472Z","shell.execute_reply":"2023-04-16T04:52:24.269426Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"df_1 = submission_df","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:52:24.774101Z","iopub.execute_input":"2023-04-16T04:52:24.775410Z","iopub.status.idle":"2023-04-16T04:52:24.781548Z","shell.execute_reply.started":"2023-04-16T04:52:24.775363Z","shell.execute_reply":"2023-04-16T04:52:24.780241Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"del models\ndel results\ndel submission_df","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:53:24.944640Z","iopub.execute_input":"2023-04-16T04:53:24.945070Z","iopub.status.idle":"2023-04-16T04:53:24.950278Z","shell.execute_reply.started":"2023-04-16T04:53:24.945034Z","shell.execute_reply":"2023-04-16T04:53:24.948962Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"gc.collect()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:52:28.190883Z","iopub.execute_input":"2023-04-16T04:52:28.191922Z","iopub.status.idle":"2023-04-16T04:52:29.652384Z","shell.execute_reply.started":"2023-04-16T04:52:28.191884Z","shell.execute_reply":"2023-04-16T04:52:29.650318Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"173297"},"metadata":{}}]},{"cell_type":"code","source":"\n# Import Modules\nimport gc\nimport os\nimport multiprocessing\nimport time\nimport numpy as np\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport tensorflow as tf\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:30:16.246270Z","iopub.execute_input":"2023-04-16T04:30:16.247262Z","iopub.status.idle":"2023-04-16T04:30:16.259172Z","shell.execute_reply.started":"2023-04-16T04:30:16.247217Z","shell.execute_reply":"2023-04-16T04:30:16.258460Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:30:16.260510Z","iopub.execute_input":"2023-04-16T04:30:16.261381Z","iopub.status.idle":"2023-04-16T04:30:16.682456Z","shell.execute_reply.started":"2023-04-16T04:30:16.261344Z","shell.execute_reply":"2023-04-16T04:30:16.681016Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"5787"},"metadata":{}}]},{"cell_type":"code","source":"# Directories and constants\nhome_dir = \"/kaggle/input/icecube-neutrinos-in-deep-ice/\"\ntest_format = home_dir + 'test/batch_{batch_id:d}.parquet'\nmodel_home = \"/kaggle/input/lstmicecubesdata/\"\n\n# Model(s)\nmodel_names = [\"4347_MAE_1-02076_bin24_pp96_n6_batch2048_epoch29.h5\",\n               \"4347_MAE_1-02039_bin24_pp96_n6_batch2048_epoch25.h5\", \n               \"4346_MAE_1-02020_bin24_pp96_n6_batch2048_epoch27.h5\"]\nmodel_weights = np.array([0.30, \n                          0.30,\n                          0.40])","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:30:16.685316Z","iopub.execute_input":"2023-04-16T04:30:16.685768Z","iopub.status.idle":"2023-04-16T04:30:16.694011Z","shell.execute_reply.started":"2023-04-16T04:30:16.685729Z","shell.execute_reply":"2023-04-16T04:30:16.692361Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Load Models\nmodels = []\nfor model_name in model_names:\n    print(f'\\n========== Model File: {model_name}')\n    \n    # Load Model\n    model_path = model_home + model_name\n    model = tf.keras.models.load_model(model_path, compile = False)\n    models.append(model)      \n    \n    # Model summary\n    model.summary()\n    \n# Get Model Parameters\npulse_count = model.inputs[0].shape[1]\nfeature_count = model.inputs[0].shape[2]\noutput_bins = model.layers[-1].weights[0].shape[-1]\nbin_num = int(np.sqrt(output_bins))\n\n# Model Parameter Summary\nprint(\"\\n==== Model Parameters\")\nprint(f\"Bin Numbers: {bin_num}\")\nprint(f\"Maximum Pulse Count: {pulse_count}\")\nprint(f\"Features Count: {feature_count}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:30:16.696114Z","iopub.execute_input":"2023-04-16T04:30:16.696683Z","iopub.status.idle":"2023-04-16T04:30:54.041838Z","shell.execute_reply.started":"2023-04-16T04:30:16.696647Z","shell.execute_reply":"2023-04-16T04:30:54.040861Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"\n========== Model File: 4347_MAE_1-02076_bin24_pp96_n6_batch2048_epoch29.h5\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 96, 6)]           0         \n_________________________________________________________________\nmasking (Masking)            (None, 96, 6)             0         \n_________________________________________________________________\nbidirectional (Bidirectional (None, 96, 384)           230400    \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 96, 384)           665856    \n_________________________________________________________________\nbidirectional_2 (Bidirection (None, 384)               665856    \n_________________________________________________________________\ndense (Dense)                (None, 256)               98560     \n_________________________________________________________________\ndense_1 (Dense)              (None, 576)               148032    \n=================================================================\nTotal params: 1,808,704\nTrainable params: 1,808,704\nNon-trainable params: 0\n_________________________________________________________________\n\n========== Model File: 4347_MAE_1-02039_bin24_pp96_n6_batch2048_epoch25.h5\n","output_type":"stream"},{"name":"stderr","text":"Exception in thread QueueFeederThread:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n    close()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n    self._close()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n    _close(self._handle)\nOSError: [Errno 9] Bad file descriptor\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n    queue_sem.release()\nValueError: semaphore or lock released too many times\n\nException in thread QueueFeederThread:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n    close()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n    self._close()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n    _close(self._handle)\nOSError: [Errno 9] Bad file descriptor\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n    queue_sem.release()\nValueError: semaphore or lock released too many times\n\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 96, 6)]           0         \n_________________________________________________________________\nmasking (Masking)            (None, 96, 6)             0         \n_________________________________________________________________\nbidirectional (Bidirectional (None, 96, 384)           230400    \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 96, 384)           665856    \n_________________________________________________________________\nbidirectional_2 (Bidirection (None, 384)               665856    \n_________________________________________________________________\ndense (Dense)                (None, 256)               98560     \n_________________________________________________________________\ndense_1 (Dense)              (None, 576)               148032    \n=================================================================\nTotal params: 1,808,704\nTrainable params: 1,808,704\nNon-trainable params: 0\n_________________________________________________________________\n\n========== Model File: 4346_MAE_1-02020_bin24_pp96_n6_batch2048_epoch27.h5\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 96, 6)]           0         \n_________________________________________________________________\nmasking (Masking)            (None, 96, 6)             0         \n_________________________________________________________________\nbidirectional (Bidirectional (None, 96, 384)           230400    \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 96, 384)           665856    \n_________________________________________________________________\nbidirectional_2 (Bidirection (None, 384)               665856    \n_________________________________________________________________\ndense (Dense)                (None, 256)               98560     \n_________________________________________________________________\ndense_1 (Dense)              (None, 576)               148032    \n=================================================================\nTotal params: 1,808,704\nTrainable params: 1,808,704\nNon-trainable params: 0\n_________________________________________________________________\n\n==== Model Parameters\nBin Numbers: 24\nMaximum Pulse Count: 96\nFeatures Count: 6\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load sensor_geometry\nsensor_geometry_df = pd.read_csv(home_dir + \"sensor_geometry.csv\")\n\n# Get Sensor Information\nsensor_x = sensor_geometry_df.x\nsensor_y = sensor_geometry_df.y\nsensor_z = sensor_geometry_df.z\n\n# Detector constants\nc_const = 0.299792458  # speed of light [m/ns]\n\n# Sensor Min / Max Coordinates\nx_min = sensor_x.min()\nx_max = sensor_x.max()\ny_min = sensor_y.min()\ny_max = sensor_y.max()\nz_min = sensor_z.min()\nz_max = sensor_z.max()\n\ndetector_length = np.sqrt((x_max - x_min)**2 + (y_max - y_min)**2 + (z_max - z_min)**2)\nt_valid_length = detector_length / c_const\n\nprint(f\"time valid length: {t_valid_length} ns\")","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:30:54.043527Z","iopub.execute_input":"2023-04-16T04:30:54.043927Z","iopub.status.idle":"2023-04-16T04:30:54.061935Z","shell.execute_reply.started":"2023-04-16T04:30:54.043888Z","shell.execute_reply":"2023-04-16T04:30:54.060500Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"time valid length: 6199.700247193777 ns\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create Azimuth Edges\nazimuth_edges = np.linspace(0, 2 * np.pi, bin_num + 1)\nprint(azimuth_edges)\n\n# Create Zenith Edges\nzenith_edges = []\nzenith_edges.append(0)\nfor bin_idx in range(1, bin_num):\n    zenith_edges.append(np.arccos(np.cos(zenith_edges[-1]) - 2 / (bin_num)))\nzenith_edges.append(np.pi)\nzenith_edges = np.array(zenith_edges)\nprint(zenith_edges)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:30:54.064170Z","iopub.execute_input":"2023-04-16T04:30:54.064875Z","iopub.status.idle":"2023-04-16T04:30:54.080273Z","shell.execute_reply.started":"2023-04-16T04:30:54.064609Z","shell.execute_reply":"2023-04-16T04:30:54.079113Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[0.         0.26179939 0.52359878 0.78539816 1.04719755 1.30899694\n 1.57079633 1.83259571 2.0943951  2.35619449 2.61799388 2.87979327\n 3.14159265 3.40339204 3.66519143 3.92699082 4.1887902  4.45058959\n 4.71238898 4.97418837 5.23598776 5.49778714 5.75958653 6.02138592\n 6.28318531]\n[0.         0.41113786 0.58568554 0.72273425 0.84106867 0.94796974\n 1.04719755 1.1410209  1.23095942 1.31811607 1.40334825 1.48736624\n 1.57079633 1.65422641 1.73824441 1.82347658 1.91063324 2.00057176\n 2.0943951  2.19362291 2.30052398 2.41885841 2.55590711 2.73045479\n 3.14159265]\n","output_type":"stream"}]},{"cell_type":"code","source":"angle_bin_zenith0 = np.tile(zenith_edges[:-1], bin_num)\nangle_bin_zenith1 = np.tile(zenith_edges[1:], bin_num)\nangle_bin_azimuth0 = np.repeat(azimuth_edges[:-1], bin_num)\nangle_bin_azimuth1 = np.repeat(azimuth_edges[1:], bin_num)\n\nangle_bin_area = (angle_bin_azimuth1 - angle_bin_azimuth0) * (np.cos(angle_bin_zenith0) - np.cos(angle_bin_zenith1))\nangle_bin_vector_sum_x = (np.sin(angle_bin_azimuth1) - np.sin(angle_bin_azimuth0)) * ((angle_bin_zenith1 - angle_bin_zenith0) / 2 - (np.sin(2 * angle_bin_zenith1) - np.sin(2 * angle_bin_zenith0)) / 4)\nangle_bin_vector_sum_y = (np.cos(angle_bin_azimuth0) - np.cos(angle_bin_azimuth1)) * ((angle_bin_zenith1 - angle_bin_zenith0) / 2 - (np.sin(2 * angle_bin_zenith1) - np.sin(2 * angle_bin_zenith0)) / 4)\nangle_bin_vector_sum_z = (angle_bin_azimuth1 - angle_bin_azimuth0) * ((np.cos(2 * angle_bin_zenith0) - np.cos(2 * angle_bin_zenith1)) / 4)\n\nangle_bin_vector_mean_x = angle_bin_vector_sum_x / angle_bin_area\nangle_bin_vector_mean_y = angle_bin_vector_sum_y / angle_bin_area\nangle_bin_vector_mean_z = angle_bin_vector_sum_z / angle_bin_area\n\nangle_bin_vector = np.zeros((1, bin_num * bin_num, 3))\nangle_bin_vector[:, :, 0] = angle_bin_vector_mean_x\nangle_bin_vector[:, :, 1] = angle_bin_vector_mean_y\nangle_bin_vector[:, :, 2] = angle_bin_vector_mean_z\n\nangle_bin_vector_unit = angle_bin_vector[0].copy()\nangle_bin_vector_unit /= np.sqrt((angle_bin_vector_unit**2).sum(axis=1).reshape((-1, 1)))","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:30:54.082215Z","iopub.execute_input":"2023-04-16T04:30:54.082621Z","iopub.status.idle":"2023-04-16T04:30:54.096549Z","shell.execute_reply.started":"2023-04-16T04:30:54.082581Z","shell.execute_reply":"2023-04-16T04:30:54.095397Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def pred_to_angle(pred, epsilon = 1e-8):\n    # Convert prediction\n    pred_vector = (pred.reshape((-1, bin_num**2, 1)) * angle_bin_vector).sum(axis = 1)\n    \n    # Normalize\n    pred_vector_norm = np.sqrt((pred_vector**2).sum(axis = 1))\n    mask = pred_vector_norm < epsilon\n    pred_vector_norm[mask] = 1\n    \n    # Assign <1, 0, 0> to very small vectors (badly predicted)\n    pred_vector /= pred_vector_norm.reshape((-1, 1))\n    pred_vector[mask] = np.array([1., 0., 0.])\n    \n    # Convert to angle\n    azimuth = np.arctan2(pred_vector[:, 1], pred_vector[:, 0])\n    azimuth[azimuth < 0] += 2 * np.pi\n    zenith = np.arccos(pred_vector[:, 2])\n    \n    # Mask bad norm predictions as 0, 0\n    azimuth[mask] = 0.\n    zenith[mask] = 0.\n    \n    return azimuth, zenith","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:30:54.098856Z","iopub.execute_input":"2023-04-16T04:30:54.099723Z","iopub.status.idle":"2023-04-16T04:30:54.111705Z","shell.execute_reply.started":"2023-04-16T04:30:54.099679Z","shell.execute_reply":"2023-04-16T04:30:54.110218Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def weighted_vector_ensemble(angles, weight):\n    # Convert angle to vector\n    vec_models = list()\n    for angle in angles:\n        az, zen = angle\n        sa = np.sin(az)\n        ca = np.cos(az)\n        sz = np.sin(zen)\n        cz = np.cos(zen)\n        vec = np.stack([sz * ca, sz * sa, cz], axis=1)\n        vec_models.append(vec)\n    vec_models = np.array(vec_models)\n\n    # Weighted-mean\n    vec_mean = (weight.reshape((-1, 1, 1)) * vec_models).sum(axis=0) / weight.sum()\n    vec_mean /= np.sqrt((vec_mean**2).sum(axis=1)).reshape((-1, 1))\n\n    # Convert vector to angle\n    zenith = np.arccos(vec_mean[:, 2])\n    azimuth = np.arctan2(vec_mean[:, 1], vec_mean[:, 0])\n    azimuth[azimuth < 0] += 2 * np.pi\n    \n    return azimuth, zenith","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:30:54.113932Z","iopub.execute_input":"2023-04-16T04:30:54.114822Z","iopub.status.idle":"2023-04-16T04:30:54.127577Z","shell.execute_reply.started":"2023-04-16T04:30:54.114777Z","shell.execute_reply":"2023-04-16T04:30:54.126231Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Placeholder\nopen_batch_dict = dict()\n\n# Read single event from batch_meta_df\ndef read_event(event_idx, batch_meta_df, pulse_count):\n    # Read metadata\n    batch_id, first_pulse_index, last_pulse_index = batch_meta_df.iloc[event_idx][[\"batch_id\", \"first_pulse_index\", \"last_pulse_index\"]].astype(\"int\")\n\n    # close past batch df\n    if batch_id - 1 in open_batch_dict.keys():\n        del open_batch_dict[batch_id - 1]\n\n    # open current batch df\n    if batch_id not in open_batch_dict.keys():\n        open_batch_dict.update({batch_id: pd.read_parquet(test_format.format(batch_id=batch_id))})\n    \n    batch_df = open_batch_dict[batch_id]\n    \n    # Read event\n    event_feature = batch_df[first_pulse_index:last_pulse_index + 1]\n    sensor_id = event_feature.sensor_id\n    \n    # Merge features into single structured array\n    dtype = [(\"time\", \"float16\"),\n             (\"charge\", \"float16\"),\n             (\"auxiliary\", \"float16\"),\n             (\"x\", \"float16\"),\n             (\"y\", \"float16\"),\n             (\"z\", \"float16\"),\n             (\"rank\", \"short\")]    \n    \n    # Create event_x\n    event_x = np.zeros(last_pulse_index - first_pulse_index + 1, dtype)\n    event_x[\"time\"] = event_feature.time.values - event_feature.time.min()\n    event_x[\"charge\"] = event_feature.charge.values\n    event_x[\"auxiliary\"] = event_feature.auxiliary.values\n    event_x[\"x\"] = sensor_geometry_df.x[sensor_id].values\n    event_x[\"y\"] = sensor_geometry_df.y[sensor_id].values\n    event_x[\"z\"] = sensor_geometry_df.z[sensor_id].values\n\n    # For long event, pick-up\n    if len(event_x) > pulse_count:\n        # Find valid time window\n        t_peak = event_x[\"time\"][event_x[\"charge\"].argmax()]\n        t_valid_min = t_peak - t_valid_length\n        t_valid_max = t_peak + t_valid_length\n        t_valid = (event_x[\"time\"] > t_valid_min) * (event_x[\"time\"] < t_valid_max)\n\n        # Rank\n        event_x[\"rank\"] = 2 * (1 - event_x[\"auxiliary\"]) + (t_valid)\n\n        # Sort by Rank and Charge (important goes to backward)\n        event_x = np.sort(event_x, order = [\"rank\", \"charge\"])\n\n        # pick-up from backward\n        event_x = event_x[-pulse_count:]\n\n        # Sort events by time \n        event_x = np.sort(event_x, order = \"time\")\n\n    return event_idx, len(event_x), event_x","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:30:54.131092Z","iopub.execute_input":"2023-04-16T04:30:54.131674Z","iopub.status.idle":"2023-04-16T04:30:54.146278Z","shell.execute_reply.started":"2023-04-16T04:30:54.131642Z","shell.execute_reply":"2023-04-16T04:30:54.144398Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Read Test Meta data\ntest_meta_df = pq.read_table(home_dir + 'test_meta.parquet').to_pandas()\nbatch_counts = test_meta_df.batch_id.value_counts().sort_index()\n\nbatch_max_index = batch_counts.cumsum()\nbatch_max_index[test_meta_df.batch_id.min() - 1] = 0\nbatch_max_index = batch_max_index.sort_index()\n\n# Support Function\ndef test_meta_df_spliter(batch_id):\n    return test_meta_df.loc[batch_max_index[batch_id - 1]:batch_max_index[batch_id] - 1]","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:30:54.147929Z","iopub.execute_input":"2023-04-16T04:30:54.148479Z","iopub.status.idle":"2023-04-16T04:30:54.172783Z","shell.execute_reply.started":"2023-04-16T04:30:54.148437Z","shell.execute_reply":"2023-04-16T04:30:54.171717Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Get Batch IDs\ntest_batch_ids = test_meta_df.batch_id.unique()\n\n# Submission Placeholders\ntest_event_id = []\ntest_azimuth = []\ntest_zenith = []\n\n# Batch Loop\nfor batch_id in test_batch_ids:\n    # Batch Meta DF\n    batch_meta_df = test_meta_df_spliter(batch_id)\n\n    # Set Pulses\n    test_x = np.zeros((len(batch_meta_df), pulse_count, feature_count), dtype = \"float16\")    \n    test_x[:, :, 2] = -1    \n\n    # Read Event Data\n    def read_event_local(event_idx):\n        return read_event(event_idx, batch_meta_df, pulse_count)\n    \n    # Multiprocess Events\n    iterator = range(len(batch_meta_df))\n    with multiprocessing.Pool() as pool:\n        for event_idx, pulsecount, event_x in pool.map(read_event_local, iterator):\n            # Features\n            test_x[event_idx, :pulsecount, 0] = event_x[\"time\"]\n            test_x[event_idx, :pulsecount, 1] = event_x[\"charge\"]\n            test_x[event_idx, :pulsecount, 2] = event_x[\"auxiliary\"]\n            test_x[event_idx, :pulsecount, 3] = event_x[\"x\"]\n            test_x[event_idx, :pulsecount, 4] = event_x[\"y\"]\n            test_x[event_idx, :pulsecount, 5] = event_x[\"z\"]\n    \n    del batch_meta_df\n    \n    # Normalize\n    test_x[:, :, 0] /= 1000  # time\n    test_x[:, :, 1] /= 300  # charge\n    test_x[:, :, 3:] /= 600  # space\n        \n    # Predict\n    pred_angles = []\n    for model in models:\n        pred_model = model.predict(test_x, verbose=0)\n        az_model, zen_model = pred_to_angle(pred_model)\n        pred_angles.append((az_model, zen_model))\n    \n    # Get Predicted Azimuth and Zenith\n    pred_azimuth, pred_zenith = weighted_vector_ensemble(pred_angles, model_weights)\n    \n    # Get Event IDs\n    event_ids = test_meta_df.event_id[test_meta_df.batch_id == batch_id].values\n    \n    # Finalize \n    for event_id, azimuth, zenith in zip(event_ids, pred_azimuth, pred_zenith):\n        if np.isfinite(azimuth) and np.isfinite(zenith):\n            test_event_id.append(int(event_id))\n            test_azimuth.append(azimuth)\n            test_zenith.append(zenith)\n        else:\n            test_event_id.append(int(event_id))\n            test_azimuth.append(0.)\n            test_zenith.append(0.)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:30:54.174972Z","iopub.execute_input":"2023-04-16T04:30:54.175552Z","iopub.status.idle":"2023-04-16T04:31:16.436211Z","shell.execute_reply.started":"2023-04-16T04:30:54.175519Z","shell.execute_reply":"2023-04-16T04:31:16.434914Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Could not load symbol cublasGetSmCountTarget from libcublas.so.11. Error: /opt/conda/bin/../lib/libcublas.so.11: undefined symbol: cublasGetSmCountTarget\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create and Save Submission.csv\nsubmission_df = pd.DataFrame({\"event_id\": test_event_id,\n                              \"azimuth\": test_azimuth,\n                              \"zenith\": test_zenith})\nsubmission_df = submission_df.sort_values(by = ['event_id'])\n#submission_df.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:31:16.438124Z","iopub.execute_input":"2023-04-16T04:31:16.438552Z","iopub.status.idle":"2023-04-16T04:31:16.451646Z","shell.execute_reply.started":"2023-04-16T04:31:16.438512Z","shell.execute_reply":"2023-04-16T04:31:16.450535Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"df_2 = submission_df","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:31:16.453564Z","iopub.execute_input":"2023-04-16T04:31:16.454697Z","iopub.status.idle":"2023-04-16T04:31:16.460313Z","shell.execute_reply.started":"2023-04-16T04:31:16.454647Z","shell.execute_reply":"2023-04-16T04:31:16.458994Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Merge the two dataframes using the event_id column key\nmerged_df = pd.merge(df_1, df_2, on='event_id')\n\n#weights of the ensamblers\nprop_1 = 0.1\nprop_2 = 0.9\n\n# Compute the weighted average of azimuth and zenith columns\nmerged_df['azimuth'] = (prop_1 * merged_df['azimuth_x']) + (prop_2 * merged_df['azimuth_y'])\nmerged_df['zenith'] = (prop_1 * merged_df['zenith_x']) + (prop_2 * merged_df['zenith_y'])\n\n# Drop the unnecessary columns\nmerged_df.drop(['azimuth_x', 'azimuth_y', 'zenith_x', 'zenith_y'], axis=1, inplace=True)\n\n# Write the merged dataframe to a new CSV file\nmerged_df.to_csv('merged.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:32:48.200477Z","iopub.execute_input":"2023-04-16T04:32:48.200911Z","iopub.status.idle":"2023-04-16T04:32:48.221507Z","shell.execute_reply.started":"2023-04-16T04:32:48.200876Z","shell.execute_reply":"2023-04-16T04:32:48.220445Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"merged_df.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T04:32:59.999668Z","iopub.execute_input":"2023-04-16T04:33:00.000112Z","iopub.status.idle":"2023-04-16T04:33:00.008357Z","shell.execute_reply.started":"2023-04-16T04:33:00.000078Z","shell.execute_reply":"2023-04-16T04:33:00.007019Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}